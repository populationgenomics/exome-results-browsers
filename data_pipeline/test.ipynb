{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "import json\n",
    "from functools import reduce\n",
    "from collections import OrderedDict\n",
    "\n",
    "import hail\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client\n",
    "import pyarrow as pa\n",
    "import pandas as pd\n",
    "from google.cloud import storage\n",
    "\n",
    "from data_pipeline.datasets.tob import helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = storage.Client()\n",
    "bucket = client.bucket(helpers.get_gcp_bucket_name())\n",
    "\n",
    "pattern = rf\"{helpers.build_analaysis_input_path(absolute_path=False)}/Genotypes/genotype_chr22.tsv\"\n",
    "chr22_genotype_files = [\n",
    "    b.name\n",
    "    for b in bucket.list_blobs()\n",
    "    if re.search(pattern, b.name)\n",
    "]\n",
    "\n",
    "pattern = rf\"{helpers.build_analaysis_input_path(absolute_path=False)}/Residuals/(.*)_chr22_log_residuals.tsv\"\n",
    "chr22_expression_files = [\n",
    "    b.name\n",
    "    for b in bucket.list_blobs()\n",
    "    if re.search(pattern, b.name)\n",
    "]\n",
    "\n",
    "cell_types = [p.split('/')[-1].split('_')[0] for p in chr22_expression_files]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dask\n",
    "Attempt to process tables from wide to long format using Dask, saving to parquet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.diskutils - INFO - Found stale lock file and directory '/home/daniel/git/exome-results-browsers/data_pipeline/dask-worker-space/worker-otm9g5wk', purging\n",
      "distributed.diskutils - INFO - Found stale lock file and directory '/home/daniel/git/exome-results-browsers/data_pipeline/dask-worker-space/worker-i1yn2sg8', purging\n"
     ]
    }
   ],
   "source": [
    "client = Client(n_workers=2, threads_per_worker=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading wide dataframe for chrom '1'\n",
      "Melting to long dataframe\n",
      "Applying column methods\n",
      "Saving to parquet\n"
     ]
    }
   ],
   "source": [
    "for chrom in list(range(1, 23)):\n",
    "    print(f\"Loading wide dataframe for chrom '{chrom}'\")\n",
    "    wide_df: dd.DataFrame = dd.read_table(\n",
    "        f\"{helpers.build_analaysis_input_path(absolute_path=True)}/Genotypes/genotype_chr{chrom}.tsv\",\n",
    "        blocksize=250e6,\n",
    "        sample=6000000\n",
    "    )\n",
    "\n",
    "    print(\"Melting to long dataframe\")\n",
    "    long_df = wide_df\\\n",
    "        .rename(columns={\"sampleid\": \"sample_id\"})\\\n",
    "        .melt(\n",
    "            id_vars=[\"sample_id\"],\n",
    "            var_name=\"snp_id\",\n",
    "            value_name=\"genotype\",\n",
    "        )\n",
    "    \n",
    "    print(\"Applying column methods\")\n",
    "    long_df[\"chrom\"] = str(chrom)\n",
    "    long_df[\"bp\"] = long_df[\"snp_id\"].apply(\n",
    "        lambda x: int(x.split(\":\")[1].split(\"_\")[0]), \n",
    "        meta=int\n",
    "    )\n",
    "    long_df[\"global_bp\"] = long_df[\"bp\"].apply(\n",
    "        lambda x: helpers.local_to_global_coordinates(x, chrom, reference=helpers.get_reference_genome()), \n",
    "        meta=int\n",
    "    )\n",
    "\n",
    "    print(\"Saving to parquet\")\n",
    "    long_df.to_parquet(\n",
    "        path=f\"{helpers.build_output_path()}/genotypes/genotypes.parquet\",\n",
    "        engine=\"fastparquet\",\n",
    "        overwrite=True if chrom == 1 else False,\n",
    "        append=False if chrom == 1 else True,\n",
    "        partition_on=\"chrom\",\n",
    "        schema={\n",
    "            \"sample_id\": pa.string(), \n",
    "            \"snp_id\": pa.string(),\n",
    "            \"chrom\": pa.string(),\n",
    "            \"bp\": pa.int64(),\n",
    "            \"global_bp\": pa.int64(),\n",
    "            \"genotype\": pa.int8(),\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_file = f\"{helpers.build_output_path()}/metadata/gene_symbol_to_id.json\".replace(f\"gs://{helpers.get_gcp_bucket_name()}/\", \"\")\n",
    "blob = bucket.get_blob(mapping_file)\n",
    "symbol_mapping = json.loads(blob.download_as_string()) if blob else None\n",
    "\n",
    "for chrom in [22]: #list(range(1, 23))[0]:\n",
    "    for cell_type in cell_types:\n",
    "        print(\"Loading wide dataframe\")\n",
    "        wide_df: dd.DataFrame = dd.read_table(\n",
    "            f\"{helpers.build_analaysis_input_path(absolute_path=True)}/Residuals/{cell_type}_chr{chrom}_*.tsv\",\n",
    "            blocksize=250e6,\n",
    "            sample=1000000\n",
    "        )\n",
    "\n",
    "        print(\"Melting to long dataframe\")\n",
    "        long_df = wide_df\\\n",
    "            .rename(columns={\"sampleid\": \"sample_id\"})\\\n",
    "            .melt(\n",
    "                id_vars=[\"sample_id\"],\n",
    "                var_name=\"gene_symbol\",\n",
    "                value_name=\"log_cpm\",\n",
    "            )\n",
    "        \n",
    "        print(\"Applying column methods\")\n",
    "        long_df[\"gene_id\"] = long_df[\"gene_symbol\"].apply(lambda x: symbol_mapping[x], meta=str)\n",
    "        long_df[\"chrom\"] = str(chrom)\n",
    "        long_df[\"cell_type_id\"] = cell_type\n",
    "\n",
    "        print(\"Saving to parquet\")\n",
    "        long_df.to_parquet(\n",
    "            path=f\"{helpers.build_output_path()}/expression/expression.parquet\",\n",
    "            engine=\"fastparquet\",\n",
    "            overwrite=True if chrom == 1 else False,\n",
    "            append=False if chrom == 1 else True,\n",
    "            partition_on=\"cell_type_id\",\n",
    "            schema={\n",
    "                \"sample_id\": pa.string(), \n",
    "                \"gene_id\": pa.string(),\n",
    "                \"cell_type_id\": pa.string(),\n",
    "                \"gene_symbol\": pa.string(),\n",
    "                \"chrom\": pa.string(),\n",
    "                \"log_cpm\": pa.float64(),\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_table(path, bucket, row_keys, verbose=False, annotations=None):\n",
    "    relative_path = path.replace(f\"gs://{bucket.name}/\", \"\")\n",
    "    blob = bucket.get_blob(relative_path)\n",
    "\n",
    "    columns = []\n",
    "    with blob.open(\"r\") as handle:\n",
    "        columns = handle.readline().split(\"\\t\")\n",
    "    columns = [c.strip() for c in columns if c and c.strip()]\n",
    "\n",
    "    row_fields = OrderedDict()\n",
    "    for col in columns:\n",
    "        row_fields[col] = hail.tstr if col in row_keys else hail.tfloat\n",
    "\n",
    "    full_path = f\"gs://{bucket.name}/{blob.name}\"\n",
    "    if verbose:\n",
    "        print(f\"Loading from path '{full_path}'\")\n",
    "        \n",
    "    table = hail.import_table(full_path, types=row_fields, delimiter=\"\\t\")\n",
    "    \n",
    "    if annotations:\n",
    "        table = table.annotate(**annotations)\n",
    "        row_fields.update({k: hail.tstr for k in annotations.keys()})\n",
    "    \n",
    "    return table.key_by(*row_keys), row_fields\n",
    "\n",
    "\n",
    "def merge_tables(tables, unify=True):\n",
    "    return reduce(lambda a, b: a.union(b, unify=unify), tables[1:], tables[0])\n",
    "\n",
    "\n",
    "def melt(df):\n",
    "    return (\n",
    "        df\n",
    "        .melt(\n",
    "            id_vars=[\"sampleid\"],\n",
    "            var_name=\"gene_symbol\",\n",
    "            value_name=\"log_residual\",\n",
    "        )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expression_table = merge_tables([\n",
    "#     read_table(\n",
    "#         path=p, \n",
    "#         bucket=bucket, \n",
    "#         row_keys=[\"sampleid\", \"cell_type_id\"], \n",
    "#         annotations={\"cell_type_id\": p.split(\"/\")[-1].split(\"_\")[0]}\n",
    "#     )[0] \n",
    "#     for p in chr22_expression_files[0:2]\n",
    "# ])\n",
    "\n",
    "# genotype_table = merge_tables([\n",
    "#     read_table(path=p, bucket=bucket, row_keys=[\"sampleid\"])[0] \n",
    "#     for p in chr22_genotype_files\n",
    "# ])\n",
    "\n",
    "# genotype_df = pd.read_table(f\"gs://{helpers.get_gcp_bucket_name()}/{chr22_genotype_files[0]}\", delimiter=\"\\t\")\n",
    "# expression_df = pd.read_table(f\"gs://{helpers.get_gcp_bucket_name()}/{chr22_expression_files[0]}\", delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:=============================>                             (2 + 2) / 4]\r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder \\\n",
    "    .master(\"local[1]\") \\\n",
    "    .appName(\"tob-wgs\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark\\\n",
    "    ._jsc\\\n",
    "    .hadoopConfiguration()\\\n",
    "    .set(\"google.cloud.auth.service.account.json.keyfile\", \"/home/daniel/keys/sa.key\")\n",
    "\n",
    "table = spark\\\n",
    "    .read\\\n",
    "    .options(header=True, delimter=\"\\t\", inferSchema=True)\\\n",
    "    .csv(f\"gs://{helpers.get_gcp_bucket_name()}/{chr22_expression_files[0]}\",\n",
    "    sep=\"\\t\"\n",
    ")\n",
    "\n",
    "genes = [c for c in table.columns if c not in [\"sampleid\", \"cell_type_id\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "\n",
    "rdd = spark.\\\n",
    "    sparkContext.\\\n",
    "    parallelize(genes)\n",
    "\n",
    "def compute_histogram(gene: str, table: DataFrame):\n",
    "    return table\\\n",
    "        .select(gene)\\\n",
    "        .rdd\\\n",
    "        .values()\\\n",
    "        .histogram(30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_histogram(gene):\n",
    "    table = spark\\\n",
    "        .read\\\n",
    "        .options(header=True, delimter=\"\\t\", inferSchema=True)\\\n",
    "        .csv(f\"gs://{helpers.get_gcp_bucket_name()}/{chr22_expression_files[0]}\",\n",
    "        sep=\"\\t\"\n",
    "    )\n",
    "\n",
    "    return table\\\n",
    "        .select(gene)\\\n",
    "        .rdd\\\n",
    "        .flatMap(lambda x: x)\\\n",
    "        .histogram(30)\n",
    "\n",
    "histograms = rdd.map(compute_histogram)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e04667d901194761864f901021819f26442928f763f4968e39becba53984e367"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 ('exome-results-browsers')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
