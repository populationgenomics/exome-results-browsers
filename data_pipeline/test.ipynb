{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re \n",
    "from functools import reduce\n",
    "from pathlib import Path\n",
    "from collections import OrderedDict\n",
    "from typing import Tuple, Dict, List\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructField, MapType, StructType, ArrayType, FloatType, IntegerType, StringType\n",
    "\n",
    "import hail\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyarrow import csv, parquet, array\n",
    "from google.cloud import storage\n",
    "\n",
    "from data_pipeline.datasets.tob import helpers\n",
    "\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = (\n",
    "    \"/Users/daniel/Library/Mobile Documents/com~apple~CloudDocs/Documents/Work/Garvan/keys/tob-wgs-browser-browser-dev-sa.json\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/21 15:48:22 WARN Utils: Your hostname, macbook.local resolves to a loopback address: 127.0.0.1; using 192.168.0.3 instead (on interface en0)\n",
      "22/04/21 15:48:22 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/homebrew/Caskroom/miniforge/base/envs/exome-results-browsers/lib/python3.10/site-packages/pyspark/jars/spark-unsafe_2.12-3.1.3.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "22/04/21 15:48:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/04/21 15:48:23 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "#Create PySpark SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[1]\") \\\n",
    "    .appName(\"tob-wgs\") \\\n",
    "    .config(\"spark.driver.memory\", \"16G\")\\\n",
    "    .config(\"spark.sql.caseSensitive\", True)\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def columns_from_file(path, bucket):\n",
    "    relative_path = resolve_path(path, bucket)\n",
    "    # blob = bucket.get_blob(relative_path)\n",
    "\n",
    "    columns = []\n",
    "    with open(relative_path, 'r') as handle:\n",
    "        columns = handle.readline().split(\"\\t\")\n",
    "    \n",
    "    return [c.strip() for c in columns if c and c.strip()]\n",
    "\n",
    "\n",
    "def resolve_path(path, bucket=None):\n",
    "    if not bucket:\n",
    "        path = path.replace(f\"gs://{helpers.get_gcp_bucket_name()}/\", \"\")\n",
    "        if \"/Users/daniel/buckets/cpg/cpg-tob-wgs-browser-dev/\" not in path:\n",
    "           return \"/Users/daniel/buckets/cpg/cpg-tob-wgs-browser-dev/\" + path\n",
    "        return path\n",
    "    else:\n",
    "        return path.replace(f\"gs://{bucket.name}/\", \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_table(path, bucket, row_keys, verbose=False, annotations=None) -> hail.Table:\n",
    "    relative_path = resolve_path(path, bucket)\n",
    "    # blob = bucket.get_blob(relative_path)\n",
    "\n",
    "    columns = []\n",
    "    with open(relative_path, 'r') as handle:\n",
    "        columns = handle.readline().split(\"\\t\")\n",
    "    columns = [c.strip() for c in columns if c and c.strip()]\n",
    "\n",
    "    row_fields = OrderedDict()\n",
    "    for col in columns:\n",
    "        row_fields[col] = hail.tstr if col in row_keys else hail.tfloat\n",
    "\n",
    "    full_path = relative_path # f\"gs://{bucket.name}/{blob.name}\"\n",
    "    if verbose:\n",
    "        print(f\"Loading from path '{full_path}'\")\n",
    "        \n",
    "    table = hail.import_table(full_path, types=row_fields, delimiter=\"\\t\")\n",
    "    \n",
    "    if annotations:\n",
    "        table = table.annotate(**annotations)\n",
    "        row_fields.update({k: hail.tstr for k in annotations.keys()})\n",
    "    \n",
    "    return table.key_by(*row_keys)\n",
    "\n",
    "\n",
    "def merge_tables(tables, unify=True) -> hail.Table:\n",
    "    return reduce(lambda a, b: a.union(b, unify=unify), tables[1:], tables[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix table functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_matrix_table(path, bucket, row_keys, verbose=False, annotations=None) -> hail.MatrixTable:\n",
    "    relative_path = resolve_path(path, bucket)\n",
    "    # blob = bucket.get_blob(relative_path)\n",
    "\n",
    "    columns = []\n",
    "    with open(relative_path, 'r') as handle:\n",
    "        columns = handle.readline().split(\"\\t\")\n",
    "    columns = [c.strip() for c in columns if c and c.strip()]\n",
    "\n",
    "    row_fields = OrderedDict()\n",
    "    for col in columns:\n",
    "        row_fields[col] = hail.tstr if col in row_keys else hail.tfloat\n",
    "\n",
    "    full_path = relative_path # f\"gs://{bucket.name}/{blob.name}\"\n",
    "    if verbose:\n",
    "        print(f\"Loading from path '{full_path}'\")\n",
    "        \n",
    "    table = hail.import_matrix_table(full_path, row_fields=row_fields, row_key=row_keys, delimiter=\"\\t\")\n",
    "    \n",
    "    if annotations:\n",
    "        table = table.annotate(**annotations)\n",
    "        row_fields.update({k: hail.tstr for k in annotations.keys()})\n",
    "    \n",
    "    return table\n",
    "\n",
    "\n",
    "def merge_matrix_tables(tables, row_join_type=\"outer\") -> hail.MatrixTable:\n",
    "    return reduce(lambda a, b: a.union_cols(b, row_join_type=row_join_type), tables[1:], tables[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['full_data/grch37/analysis_output/kccg/Genotypes/genotype_chr1.tsv', 'full_data/grch37/analysis_output/kccg/Genotypes/genotype_chr10.tsv', 'full_data/grch37/analysis_output/kccg/Genotypes/genotype_chr11.tsv', 'full_data/grch37/analysis_output/kccg/Genotypes/genotype_chr12.tsv', 'full_data/grch37/analysis_output/kccg/Genotypes/genotype_chr13.tsv', 'full_data/grch37/analysis_output/kccg/Genotypes/genotype_chr14.tsv', 'full_data/grch37/analysis_output/kccg/Genotypes/genotype_chr15.tsv', 'full_data/grch37/analysis_output/kccg/Genotypes/genotype_chr16.tsv', 'full_data/grch37/analysis_output/kccg/Genotypes/genotype_chr17.tsv', 'full_data/grch37/analysis_output/kccg/Genotypes/genotype_chr18.tsv', 'full_data/grch37/analysis_output/kccg/Genotypes/genotype_chr19.tsv', 'full_data/grch37/analysis_output/kccg/Genotypes/genotype_chr2.tsv', 'full_data/grch37/analysis_output/kccg/Genotypes/genotype_chr20.tsv', 'full_data/grch37/analysis_output/kccg/Genotypes/genotype_chr21.tsv', 'full_data/grch37/analysis_output/kccg/Genotypes/genotype_chr22.tsv', 'full_data/grch37/analysis_output/kccg/Genotypes/genotype_chr3.tsv', 'full_data/grch37/analysis_output/kccg/Genotypes/genotype_chr4.tsv', 'full_data/grch37/analysis_output/kccg/Genotypes/genotype_chr5.tsv', 'full_data/grch37/analysis_output/kccg/Genotypes/genotype_chr6.tsv', 'full_data/grch37/analysis_output/kccg/Genotypes/genotype_chr7.tsv', 'full_data/grch37/analysis_output/kccg/Genotypes/genotype_chr8.tsv', 'full_data/grch37/analysis_output/kccg/Genotypes/genotype_chr9.tsv']\n",
      "['full_data/grch37/analysis_output/kccg/Residuals/bin_chr1_log_residuals.tsv', 'full_data/grch37/analysis_output/kccg/Residuals/bmem_chr1_log_residuals.tsv', 'full_data/grch37/analysis_output/kccg/Residuals/cd4et_chr1_log_residuals.tsv', 'full_data/grch37/analysis_output/kccg/Residuals/cd4nc_chr1_log_residuals.tsv', 'full_data/grch37/analysis_output/kccg/Residuals/cd4sox4_chr1_log_residuals.tsv', 'full_data/grch37/analysis_output/kccg/Residuals/cd8et_chr1_log_residuals.tsv', 'full_data/grch37/analysis_output/kccg/Residuals/cd8nc_chr1_log_residuals.tsv', 'full_data/grch37/analysis_output/kccg/Residuals/cd8s100b_chr1_log_residuals.tsv', 'full_data/grch37/analysis_output/kccg/Residuals/dc_chr1_log_residuals.tsv', 'full_data/grch37/analysis_output/kccg/Residuals/monoc_chr1_log_residuals.tsv', 'full_data/grch37/analysis_output/kccg/Residuals/mononc_chr1_log_residuals.tsv', 'full_data/grch37/analysis_output/kccg/Residuals/nk_chr1_log_residuals.tsv', 'full_data/grch37/analysis_output/kccg/Residuals/nkr_chr1_log_residuals.tsv', 'full_data/grch37/analysis_output/kccg/Residuals/plasma_chr1_log_residuals.tsv']\n"
     ]
    }
   ],
   "source": [
    "client = storage.Client()\n",
    "bucket = client.bucket(helpers.get_gcp_bucket_name())\n",
    "\n",
    "pattern = rf\"{helpers.build_analaysis_input_path(absolute_path=False)}/Genotypes/genotype_(.*).tsv\"\n",
    "genotype_files = [\n",
    "    b.name\n",
    "    for b in bucket.list_blobs()\n",
    "    if re.search(pattern, b.name)\n",
    "]\n",
    "print(genotype_files)\n",
    "\n",
    "# Data for a cell type in the different chromosome files are the same. Save time a memory by loading one.\n",
    "pattern = rf\"{helpers.build_analaysis_input_path(absolute_path=False)}/Residuals/(.*)_chr1_log_residuals.tsv\"\n",
    "expression_files = [\n",
    "    b.name\n",
    "    for b in bucket.list_blobs()\n",
    "    if re.search(pattern, b.name)\n",
    "]\n",
    "print(expression_files)\n",
    "\n",
    "cell_types = [p.split('/')[-1].split('_')[0] for p in expression_files]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expression summary pre-compute\n",
    "Pre-compute the histogram and statistics for each gene in each cell-type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataframe(path: str, index_col=\"sampleid\") -> pd.DataFrame:\n",
    "    df = pd.read_table(path, header=0, index_col=False, sep=\"\\t\")\n",
    "    df.set_index(index_col, inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def generate_summary_statistics(df: pd.DataFrame, bins: int = 30, range: Tuple[float, float] = (-1, 1)) -> pd.DataFrame:\n",
    "    histograms = df.apply(\n",
    "        lambda x: compute_histogram(x, bins, range), \n",
    "        axis=1\n",
    "    )\n",
    "    statistics = df.apply(\n",
    "        lambda x: {\n",
    "            \"min\": float(np.min(x)),\n",
    "            \"max\": float(np.max(x)),\n",
    "            \"median\": float(np.median(x)),\n",
    "            \"mean\": float(np.mean(x)),\n",
    "            \"q1\": float(np.quantile(x, q=0.25)),\n",
    "            \"q3\": float(np.quantile(x, q=0.75)),\n",
    "            \"iqr\": float(np.quantile(x, q=0.75) - np.quantile(x, q=0.25)),\n",
    "            \"iqr_min\": float(np.quantile(x, q=0.25) - 1.5 * (np.quantile(x, q=0.75) - np.quantile(x, q=0.25))),\n",
    "            \"iqr_max\": float(np.quantile(x, q=0.75) + 1.5 * (np.quantile(x, q=0.75) - np.quantile(x, q=0.25))),\n",
    "        }, \n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    return pd.DataFrame(\n",
    "        {\n",
    "            \"histogram\": histograms,\n",
    "            \"statistics\": statistics,\n",
    "        },\n",
    "    )\n",
    "\n",
    "\n",
    "def compute_histogram(data: pd.Series, bins: int = 30, range: Tuple[float, float] = (-1, 1)) -> Dict[str, List[int]]:\n",
    "    counts, bin_edges = np.histogram(data, bins=bins, range=range)\n",
    "    return {\"counts\": [int(x) for x in counts.tolist()], \"bin_edges\": [float(x) for x in bin_edges.tolist()]}\n",
    "\n",
    "\n",
    "def merge_rows(group):\n",
    "    data = {\n",
    "        k: v \n",
    "        for k, v in zip(\n",
    "            [r[\"cell_type_id\"] for _, r in group.iterrows()], \n",
    "            [{\"histogram\": r[\"histogram\"], \"statistics\": r[\"statistics\"]} for _, r in group.iterrows()]\n",
    "        )\n",
    "    }\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        \"gene_id\": [group[\"gene_id\"].values[0]],\n",
    "        \"gene_symbol\": [group[\"gene_symbol\"].values[0]],\n",
    "        \"cell_type_ids\": [data]\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(name=\"gene_id\", dataType=StringType(), nullable=False),\n",
    "    StructField(name=\"gene_symbol\", dataType=StringType(), nullable=False),\n",
    "    StructField(name=\"cell_type_id\", dataType=StringType(), nullable=False),\n",
    "    StructField(\n",
    "        name=\"histogram\",\n",
    "        dataType=StructType([\n",
    "            StructField(name=\"counts\", dataType=ArrayType(IntegerType(), containsNull=False), nullable=False),\n",
    "            StructField(name=\"bin_edges\", dataType=ArrayType(FloatType(), containsNull=False), nullable=False),\n",
    "        ]),\n",
    "        nullable=False,\n",
    "    ),\n",
    "    StructField(\n",
    "        name=\"statistics\",\n",
    "        dataType=StructType([\n",
    "            StructField(name=\"min\", dataType=FloatType(), nullable=False),\n",
    "            StructField(name=\"max\", dataType=FloatType(), nullable=False),\n",
    "            StructField(name=\"mean\", dataType=FloatType(), nullable=False),\n",
    "            StructField(name=\"median\", dataType=FloatType(), nullable=False),\n",
    "            StructField(name=\"q1\", dataType=FloatType(), nullable=False),\n",
    "            StructField(name=\"q3\", dataType=FloatType(), nullable=False),\n",
    "            StructField(name=\"iqr\", dataType=FloatType(), nullable=False),\n",
    "            StructField(name=\"iqr_min\", dataType=FloatType(), nullable=False),\n",
    "            StructField(name=\"iqr_max\", dataType=FloatType(), nullable=False),\n",
    "        ]),\n",
    "        nullable=False,\n",
    "    )\n",
    "])\n",
    "\n",
    "# cell_types = ['bin']\n",
    "for (index, cell_type) in enumerate(cell_types):\n",
    "    print(f\"Processing files for cell type '{cell_type}'\")\n",
    "    \n",
    "    paths = [resolve_path(p, bucket=None) for p in expression_files if cell_type in p]\n",
    "    if not paths: continue\n",
    "\n",
    "    df: pd.DataFrame = pd.concat([read_dataframe(p) for p in paths], axis=0, join=\"outer\").transpose()\n",
    "    \n",
    "    aggregate_df: pd.DataFrame = generate_summary_statistics(df)\n",
    "    aggregate_df[\"gene_id\"] = df.index  # TODO: set ensembl gene ids \n",
    "    aggregate_df[\"gene_symbol\"] = df.index\n",
    "    aggregate_df[\"cell_type_id\"] = cell_type\n",
    "    aggregate_df = aggregate_df[[\"gene_id\", \"gene_symbol\", \"cell_type_id\", \"histogram\", \"statistics\"]]\n",
    "\n",
    "    data = spark.createDataFrame(data=aggregate_df, schema=schema)\n",
    "    data\\\n",
    "        .write\\\n",
    "        .mode(\"append\" if index > 0 else \"overwrite\")\\\n",
    "        .partitionBy(\"cell_type_id\")\\\n",
    "        .parquet(\"/Users/daniel/Desktop/expression_summary.parquet\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TSV to Parquet\n",
    "Convert genotypes and expression `tsv` files to parquet for more efficient column-based querying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data for chrom '1'\n",
      "Writing to parquet for chrom '1'\n",
      "Reading data for chrom '2'\n",
      "Writing to parquet for chrom '2'\n",
      "Reading data for chrom '3'\n",
      "Writing to parquet for chrom '3'\n",
      "Reading data for chrom '4'\n",
      "Writing to parquet for chrom '4'\n",
      "Reading data for chrom '5'\n",
      "Writing to parquet for chrom '5'\n",
      "Reading data for chrom '6'\n",
      "Writing to parquet for chrom '6'\n",
      "Reading data for chrom '7'\n",
      "Writing to parquet for chrom '7'\n",
      "Reading data for chrom '8'\n",
      "Writing to parquet for chrom '8'\n",
      "Reading data for chrom '9'\n",
      "Writing to parquet for chrom '9'\n",
      "Reading data for chrom '10'\n",
      "Writing to parquet for chrom '10'\n",
      "Reading data for chrom '11'\n",
      "Writing to parquet for chrom '11'\n",
      "Reading data for chrom '12'\n",
      "Writing to parquet for chrom '12'\n",
      "Reading data for chrom '13'\n",
      "Writing to parquet for chrom '13'\n",
      "Reading data for chrom '14'\n",
      "Writing to parquet for chrom '14'\n",
      "Reading data for chrom '15'\n",
      "Writing to parquet for chrom '15'\n",
      "Reading data for chrom '16'\n",
      "Writing to parquet for chrom '16'\n",
      "Reading data for chrom '17'\n",
      "Writing to parquet for chrom '17'\n",
      "Reading data for chrom '18'\n",
      "Writing to parquet for chrom '18'\n",
      "Reading data for chrom '19'\n",
      "Writing to parquet for chrom '19'\n",
      "Reading data for chrom '20'\n",
      "Writing to parquet for chrom '20'\n",
      "Reading data for chrom '21'\n",
      "Writing to parquet for chrom '21'\n",
      "Reading data for chrom '22'\n",
      "Writing to parquet for chrom '22'\n"
     ]
    }
   ],
   "source": [
    "for chrom in range(1, 23):\n",
    "    print(f\"Reading data for chrom '{chrom}'\")\n",
    "    file = resolve_path(\n",
    "        f\"{helpers.build_analaysis_input_path()}/Genotypes/genotype_chr{chrom}.tsv\", \n",
    "        bucket=None\n",
    "    )\n",
    "\n",
    "    types = {\n",
    "        col: 'str' if col == \"sampleid\" else 'int8'\n",
    "        for col in columns_from_file(file, bucket=None)\n",
    "    }\n",
    "\n",
    "    table = csv.read_csv(\n",
    "        file,\n",
    "        read_options=csv.ReadOptions(block_size=7e6),\n",
    "        parse_options=csv.ParseOptions(delimiter=\"\\t\"),\n",
    "        convert_options=csv.ConvertOptions(column_types=types)\n",
    "    )\n",
    "    table = table.rename_columns([c.replace(\":\", \"_\") for c in table.column_names])\n",
    "\n",
    "    print(f\"Writing to parquet for chrom '{chrom}'\")\n",
    "    root_path = Path(f\"/Users/daniel/Desktop/genotypes/genotypes_chr{chrom}.parquet\")\n",
    "    \n",
    "    # Write a dataset and collect metadata information of all written files\n",
    "    metadata_collector = []\n",
    "    parquet.write_to_dataset(table, root_path, metadata_collector=metadata_collector)\n",
    "\n",
    "    # Write the ``_common_metadata`` parquet file without row groups statistics\n",
    "    parquet.write_metadata(table.schema, root_path / '_common_metadata')\n",
    "\n",
    "    # Write the ``_metadata`` parquet file with row groups statistics of all files\n",
    "    parquet.write_metadata(\n",
    "        table.schema, \n",
    "        root_path / '_metadata',\n",
    "        metadata_collector=metadata_collector\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data for cell type 'bin'\n",
      "Writing to parquet for cell type 'bin'\n",
      "Reading data for cell type 'bmem'\n",
      "Writing to parquet for cell type 'bmem'\n",
      "Reading data for cell type 'cd4et'\n",
      "Writing to parquet for cell type 'cd4et'\n",
      "Reading data for cell type 'cd4nc'\n",
      "Writing to parquet for cell type 'cd4nc'\n",
      "Reading data for cell type 'cd4sox4'\n",
      "Writing to parquet for cell type 'cd4sox4'\n",
      "Reading data for cell type 'cd8et'\n",
      "Writing to parquet for cell type 'cd8et'\n",
      "Reading data for cell type 'cd8nc'\n",
      "Writing to parquet for cell type 'cd8nc'\n",
      "Reading data for cell type 'cd8s100b'\n",
      "Writing to parquet for cell type 'cd8s100b'\n",
      "Reading data for cell type 'dc'\n",
      "Writing to parquet for cell type 'dc'\n",
      "Reading data for cell type 'monoc'\n",
      "Writing to parquet for cell type 'monoc'\n",
      "Reading data for cell type 'mononc'\n",
      "Writing to parquet for cell type 'mononc'\n",
      "Reading data for cell type 'nk'\n",
      "Writing to parquet for cell type 'nk'\n",
      "Reading data for cell type 'nkr'\n",
      "Writing to parquet for cell type 'nkr'\n",
      "Reading data for cell type 'plasma'\n",
      "Writing to parquet for cell type 'plasma'\n"
     ]
    }
   ],
   "source": [
    "for cell_type in cell_types:\n",
    "    print(f\"Reading data for cell type '{cell_type}'\")\n",
    "    file = resolve_path(\n",
    "        f\"{helpers.build_analaysis_input_path()}/Residuals/{cell_type}_chr1_log_residuals.tsv\", \n",
    "        bucket=None\n",
    "    )\n",
    "\n",
    "    types = {\n",
    "        col: 'str' if col == \"sampleid\" else 'float64'\n",
    "        for col in columns_from_file(file, bucket=None)\n",
    "    }\n",
    "\n",
    "    table = csv.read_csv(\n",
    "        file,\n",
    "        parse_options=csv.ParseOptions(delimiter=\"\\t\"),\n",
    "        convert_options=csv.ConvertOptions(column_types=types)\n",
    "    )\n",
    "    table = table.rename_columns([\n",
    "        c.upper().replace(\":\", \"_\").replace(\"-\", \"_\").replace(\".\", \"_\") \n",
    "        if c != \"sampleid\" else c \n",
    "        for c in table.column_names\n",
    "    ])\n",
    "\n",
    "    print(f\"Writing to parquet for cell type '{cell_type}'\")\n",
    "    root_path = Path(f\"/Users/daniel/Desktop/expression/{cell_type}.parquet\")\n",
    "    \n",
    "    # Write a dataset and collect metadata information of all written files\n",
    "    metadata_collector = []\n",
    "    parquet.write_to_dataset(table, root_path, metadata_collector=metadata_collector)\n",
    "\n",
    "    # Write the ``_common_metadata`` parquet file without row groups statistics\n",
    "    parquet.write_metadata(table.schema, root_path / '_common_metadata')\n",
    "\n",
    "    # Write the ``_metadata`` parquet file with row groups statistics of all files\n",
    "    parquet.write_metadata(\n",
    "        table.schema, \n",
    "        root_path / '_metadata',\n",
    "        metadata_collector=metadata_collector\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"/Users/daniel/Desktop/genotypes.parquet\"\n",
    "column = '22_17302763_C'\n",
    "\n",
    "df = spark.read.parquet(\"/Users/daniel/Desktop/genotypes/genotypes_chr22.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sampleid</th>\n",
       "      <th>22_17302763_C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1_1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2_2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3_3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4_4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6_6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sampleid  22_17302763_C\n",
       "0      1_1              0\n",
       "1      2_2              0\n",
       "2      3_3              0\n",
       "3      4_4              1\n",
       "4      6_6              1"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_parquet(\"/Users/daniel/Desktop/genotypes/genotypes_chr22.parquet\", columns=[\"sampleid\", column])\n",
    "\n",
    "# rows = df.select(\"sampleid\").filter(f\"{column} > 0\")\n",
    "# rows.collect()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Association effect pre-compute\n",
    "Pre-compute the histogram and statistics for each genotype for each unique eQTL with an FDR below 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(name=\"association_id\", dataType=StringType(), nullable=False),\n",
    "    StructField(name=\"cell_type_id\", dataType=StringType(), nullable=False),\n",
    "    StructField(\n",
    "        name=\"genotypes\",\n",
    "        dataType=MapType(\n",
    "            keyType=StringType(),\n",
    "            valueType=StructType([\n",
    "                StructField(\n",
    "                    name=\"histogram\",\n",
    "                    dataType=StructType([\n",
    "                        StructField(name=\"counts\", dataType=ArrayType(IntegerType(), containsNull=False), nullable=False),\n",
    "                        StructField(name=\"bin_edges\", dataType=ArrayType(FloatType(), containsNull=False), nullable=False),\n",
    "                    ]),\n",
    "                    nullable=False,\n",
    "                ),\n",
    "                StructField(\n",
    "                    name=\"statistics\",\n",
    "                    dataType=StructType([\n",
    "                        StructField(name=\"min\", dataType=FloatType(), nullable=False),\n",
    "                        StructField(name=\"max\", dataType=FloatType(), nullable=False),\n",
    "                        StructField(name=\"mean\", dataType=FloatType(), nullable=False),\n",
    "                        StructField(name=\"median\", dataType=FloatType(), nullable=False),\n",
    "                        StructField(name=\"q1\", dataType=FloatType(), nullable=False),\n",
    "                        StructField(name=\"q3\", dataType=FloatType(), nullable=False),\n",
    "                        StructField(name=\"iqr\", dataType=FloatType(), nullable=False),\n",
    "                        StructField(name=\"iqr_min\", dataType=FloatType(), nullable=False),\n",
    "                        StructField(name=\"iqr_max\", dataType=FloatType(), nullable=False),\n",
    "                    ]),\n",
    "                    nullable=False,\n",
    "                ),\n",
    "            ]),\n",
    "            valueContainsNull=True,\n",
    "        ),\n",
    "        nullable=False\n",
    "    )\n",
    "])\n",
    "\n",
    "\n",
    "def statistics(x):\n",
    "    if not x.size:\n",
    "        return None\n",
    "\n",
    "    return {\n",
    "        \"min\": float(np.min(x)),\n",
    "        \"max\": float(np.max(x)),\n",
    "        \"median\": float(np.median(x)),\n",
    "        \"mean\": float(np.mean(x)),\n",
    "        \"q1\": float(np.quantile(x, q=0.25)),\n",
    "        \"q3\": float(np.quantile(x, q=0.75)),\n",
    "        \"iqr\": float(np.quantile(x, q=0.75) - np.quantile(x, q=0.25)),\n",
    "        \"iqr_min\": float(np.quantile(x, q=0.25) - 1.5 * (np.quantile(x, q=0.75) - np.quantile(x, q=0.25))),\n",
    "        \"iqr_max\": float(np.quantile(x, q=0.75) + 1.5 * (np.quantile(x, q=0.75) - np.quantile(x, q=0.25))),\n",
    "    }\n",
    "\n",
    "\n",
    "def histogram(x, bins=30, range=(-1,1)):\n",
    "    if not x.size:\n",
    "        return None\n",
    "    return np.histogram(x, bins=bins, range=range)\n",
    "\n",
    "\n",
    "def compute_summary(row, genotypes_df, expression_cache):\n",
    "    genotype_id = f\"{row.chrom}:{row.bp}_{row.a2}\"\n",
    "    gene_id = row.gene_symbol\n",
    "    expression_df = expression_cache[row.cell_type_id]\n",
    "\n",
    "    genotypes = {}\n",
    "    for alt_count, genotype in enumerate([f\"{row.a1}{row.a1}\", f\"{row.a1}{row.a2}\", f\"{row.a2}{row.a2}\"]):\n",
    "        samples = genotypes_df.loc[genotype_id][genotypes_df.loc[genotype_id] == alt_count].index.tolist()\n",
    "        expression_values = expression_df.loc[[gene_id]][expression_df.columns.intersection(samples)]\n",
    "        data = {\n",
    "            \"histogram\": histogram(expression_values.values),\n",
    "            \"statistics\": statistics(expression_values.values)\n",
    "        } \n",
    "        genotypes[genotype] = data\n",
    "\n",
    "    row = row[[\"association_id\", \"cell_type_id\", \"chrom\"]]\n",
    "    row[\"genotypes\"] = genotypes\n",
    "    return row"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e04667d901194761864f901021819f26442928f763f4968e39becba53984e367"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 ('exome-results-browsers')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
